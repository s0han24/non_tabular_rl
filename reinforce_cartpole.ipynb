{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "84203218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "seed = 2024\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "discount_factor = 1\n",
    "num_episodes = 1000\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d3f0282c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class policy_network(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.FC = nn.Sequential(\n",
    "            nn.Linear(input_size, 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, output_size)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.FC(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2f6d124e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_state(state):\n",
    "    normalized_state = torch.as_tensor(state, dtype=torch.float32, device=device)\n",
    "\n",
    "    return normalized_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7c4e8421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(rewards):\n",
    "\n",
    "    t_steps = np.arange(len(rewards))\n",
    "    r = rewards * discount_factor ** t_steps # Compute discounted rewards for each time step\n",
    "\n",
    "    # Compute the discounted cumulative sum in reverse order and then reverse it again to restore the original order.\n",
    "    r = np.cumsum(r[::-1])[::-1] / discount_factor ** t_steps\n",
    "\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c0822ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(log_probs, returns):\n",
    "\n",
    "    loss = []\n",
    "    for log_prob, returns in zip(log_probs, returns):\n",
    "        loss.append(log_prob * returns)\n",
    "\n",
    "    return -torch.stack(loss).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e28d4f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_results(rewards, losses):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "    ax1.plot(rewards)\n",
    "    ax1.set_xlabel('Episode')\n",
    "    ax1.set_ylabel('Episode Reward')\n",
    "    ax1.set_title('Training Rewards')\n",
    "    ax1.grid(True)\n",
    "\n",
    "    ax2.plot(losses)\n",
    "    ax2.set_xlabel('Episode')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.set_title('Training Losses')\n",
    "    ax2.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def test_and_create_gif(policy, gif_filename='cartpole_test.gif'):\n",
    "    test_env = gym.make(\"CartPole-v1\", max_episode_steps=500, render_mode=\"rgb_array\")\n",
    "    test_env = gym.wrappers.NormalizeObservation(test_env)\n",
    "\n",
    "    frames = []\n",
    "    total_reward = 0\n",
    "\n",
    "    state, _ = test_env.reset(seed=seed)\n",
    "    done = False\n",
    "    truncation = False\n",
    "\n",
    "    print(\"Testing trained policy and creating GIF...\")\n",
    "\n",
    "    while not done and not truncation:\n",
    "        frame = test_env.render()\n",
    "        frames.append(Image.fromarray(frame))\n",
    "\n",
    "        state_tensor = preprocess_state(state)\n",
    "        action_probs = policy(state_tensor)\n",
    "        action = torch.argmax(action_probs, dim=0).item()\n",
    "\n",
    "        state, reward, done, truncation, _ = test_env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "    test_env.close()\n",
    "\n",
    "    if frames:\n",
    "        frames[0].save(\n",
    "            gif_filename,\n",
    "            save_all=True,\n",
    "            append_images=frames[1:],\n",
    "            duration=50,  # 50ms per frame\n",
    "            loop=0\n",
    "        )\n",
    "        print(f\"GIF saved as '{gif_filename}' with {len(frames)} frames\")\n",
    "        print(f\"Test episode reward: {total_reward}\")\n",
    "    else:\n",
    "        print(\"No frames captured for GIF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e99bb4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env):\n",
    "    # Define the policy network and its optimizer\n",
    "    policy = policy_network(input_size=4, output_size=2).to(device)\n",
    "    optimizer = torch.optim.Adam(policy.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Lists to store episode rewards and losses for plotting\n",
    "    episode_rewards = []\n",
    "    episode_losses = []\n",
    "\n",
    "    # Main training loop that loops over each episode\n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        state, _ = env.reset(seed=seed) # Reset the environment and get the initial state\n",
    "\n",
    "        # Initialize empty lists to store log probabilities and rewards for the episode\n",
    "        log_probs = []\n",
    "        episode_reward = []\n",
    "\n",
    "        # Loop until the episode is done\n",
    "        while True:\n",
    "            state = preprocess_state(state) # Preprocesses the state to convert it to tensor\n",
    "            action_probs = policy(state) # Get action probabilities from the policy network\n",
    "\n",
    "            # Sample an action from the action probabilities\n",
    "            dist = torch.distributions.Categorical(action_probs)  # Create a categorical distribution for the action probabilities\n",
    "            action = dist.sample()\n",
    "\n",
    "            # Compute the log probability of the sampled action\n",
    "            log_prob = dist.log_prob(action)\n",
    "            log_probs.append(log_prob)\n",
    "\n",
    "            # Take a step in the environment\n",
    "            next_state, reward, done, truncation, _ = env.step(action.item())\n",
    "            episode_reward.append(reward)\n",
    "\n",
    "            state = next_state # Update the current state\n",
    "\n",
    "            if done or truncation: # if the episode is done or truncated\n",
    "                returns = compute_returns(episode_reward) # Compute the returns (discounted sum of rewards) for the episode\n",
    "                loss = compute_loss(log_probs, returns)  # Compute the loss for the episode using the log probabilities and returns\n",
    "\n",
    "                optimizer.zero_grad() # Zero the gradients of the optimizer\n",
    "                loss.backward() # Backpropagate the loss\n",
    "                optimizer.step() # Update the parameters of the policy network\n",
    "\n",
    "                total_reward = sum(episode_reward) # Compute the total reward for the episode\n",
    "                episode_rewards.append(total_reward)\n",
    "                episode_losses.append(loss.item())\n",
    "\n",
    "                if episode % 50 == 0:\n",
    "                    print(f\"Episode {episode}, Reward: {total_reward:.2f}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "                break # Exit the episode loop\n",
    "\n",
    "    # Plot training results\n",
    "    plot_training_results(episode_rewards, episode_losses)\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a70a6786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, batch_size=5000, epochs=50):\n",
    "\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    n_acts = env.action_space.n\n",
    "\n",
    "    # make core of policy network\n",
    "    logits_net = policy_network(input_size=obs_dim, output_size=n_acts).to(device)\n",
    "\n",
    "    # make function to compute action distribution\n",
    "    def get_policy(obs):\n",
    "        logits = logits_net(obs)\n",
    "        return torch.distributions.Categorical(logits=logits)\n",
    "\n",
    "    # make action selection function (outputs int actions, sampled from policy)\n",
    "    def get_action(obs):\n",
    "        return get_policy(obs).sample().item()\n",
    "\n",
    "    # make loss function whose gradient, for the right data, is policy gradient\n",
    "    def compute_loss(obs, act, weights):\n",
    "        logp = get_policy(obs).log_prob(act)\n",
    "        return -(logp * weights).mean()\n",
    "\n",
    "    # make optimizer\n",
    "    optimizer = torch.optim.Adam(logits_net.parameters(), lr=learning_rate)\n",
    "\n",
    "    # collect data for plot_training_results\n",
    "    rewards = []\n",
    "    losses = []\n",
    "\n",
    "    # for training policy\n",
    "    def train_one_epoch():\n",
    "        # make some empty lists for logging.\n",
    "        batch_obs = []          # for observations\n",
    "        batch_acts = []         # for actions\n",
    "        batch_weights = []      # for R(tau) weighting in policy gradient\n",
    "        batch_rets = []         # for measuring episode  _ = env.step(act)returns\n",
    "        batch_lens = []         # for measuring episode lengths\n",
    "\n",
    "        # reset episode-specific variables\n",
    "        obs, _ = env.reset()       # first obs comes from starting distribution\n",
    "        done = False            # signal from environment that episode is over\n",
    "        ep_rews = []            # list for rewards accrued throughout ep\n",
    "\n",
    "        # render first episode of each epoch\n",
    "        finished_rendering_this_epoch = False\n",
    "\n",
    "        # collect experience by acting in the environment with current policy\n",
    "        while True:\n",
    "            # save obs\n",
    "            batch_obs.append(obs.copy())\n",
    "\n",
    "            # act in the environment\n",
    "            act = get_action(torch.as_tensor(obs, dtype=torch.float32).to(device))\n",
    "            obs, rew, done, truncation, _ = env.step(act)\n",
    "\n",
    "            # save action, reward\n",
    "            batch_acts.append(act)\n",
    "            ep_rews.append(rew)\n",
    "\n",
    "            if done or truncation:\n",
    "                # if episode is over, record info about episode\n",
    "                ep_ret, ep_len = sum(ep_rews), len(ep_rews)\n",
    "                batch_rets.append(ep_ret)\n",
    "                batch_lens.append(ep_len)\n",
    "\n",
    "                # the weight for each logprob(a|s) is R(tau)\n",
    "                batch_weights += [ep_ret] * ep_len\n",
    "\n",
    "                # reset episode-specific variables\n",
    "                obs, _ = env.reset()\n",
    "                done, ep_rews = False, []\n",
    "\n",
    "                # won't render again this epoch\n",
    "                finished_rendering_this_epoch = True\n",
    "\n",
    "                # end experience loop if we have enough of it\n",
    "                if len(batch_obs) > batch_size:\n",
    "                    break\n",
    "\n",
    "        # take a single policy gradient update step\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss = compute_loss(obs=torch.as_tensor(batch_obs, dtype=torch.float32).to(device),\n",
    "                                  act=torch.as_tensor(batch_acts, dtype=torch.int32).to(device),\n",
    "                                  weights=torch.as_tensor(batch_weights, dtype=torch.float32).to(device)\n",
    "                                  )\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        return batch_loss, batch_rets, batch_lens\n",
    "\n",
    "    # training loop _ = env.step(act)\n",
    "    for i in range(epochs):\n",
    "        batch_loss, batch_rets, batch_lens = train_one_epoch()\n",
    "        print('epoch: %3d \\t loss: %.3f \\t return: %.3f \\t ep_len: %.3f'%\n",
    "                (i, batch_loss, np.mean(batch_rets), np.mean(batch_lens)))\n",
    "        rewards += batch_rets\n",
    "        losses.append(batch_loss.item())\n",
    "\n",
    "    return rewards, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9616e9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting REINFORCE training...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   0 \t loss: 18.776 \t return: 21.947 \t ep_len: 21.947\n",
      "epoch:   1 \t loss: 19.666 \t return: 22.061 \t ep_len: 22.061\n",
      "epoch:   2 \t loss: 17.497 \t return: 20.755 \t ep_len: 20.755\n",
      "epoch:   3 \t loss: 18.121 \t return: 21.225 \t ep_len: 21.225\n",
      "epoch:   4 \t loss: 19.795 \t return: 22.466 \t ep_len: 22.466\n",
      "epoch:   5 \t loss: 19.799 \t return: 22.393 \t ep_len: 22.393\n",
      "epoch:   6 \t loss: 20.667 \t return: 23.032 \t ep_len: 23.032\n",
      "epoch:   7 \t loss: 18.531 \t return: 21.599 \t ep_len: 21.599\n",
      "epoch:   8 \t loss: 20.713 \t return: 22.777 \t ep_len: 22.777\n",
      "epoch:   9 \t loss: 22.305 \t return: 25.055 \t ep_len: 25.055\n",
      "epoch:  10 \t loss: 18.967 \t return: 22.267 \t ep_len: 22.267\n",
      "epoch:  11 \t loss: 23.386 \t return: 25.146 \t ep_len: 25.146\n",
      "epoch:  12 \t loss: 18.779 \t return: 22.439 \t ep_len: 22.439\n",
      "epoch:  13 \t loss: 20.162 \t return: 23.905 \t ep_len: 23.905\n",
      "epoch:  14 \t loss: 21.426 \t return: 24.222 \t ep_len: 24.222\n",
      "epoch:  15 \t loss: 20.843 \t return: 24.266 \t ep_len: 24.266\n",
      "epoch:  16 \t loss: 22.220 \t return: 24.159 \t ep_len: 24.159\n",
      "epoch:  17 \t loss: 23.277 \t return: 25.333 \t ep_len: 25.333\n",
      "epoch:  18 \t loss: 24.499 \t return: 25.308 \t ep_len: 25.308\n",
      "epoch:  19 \t loss: 21.412 \t return: 24.183 \t ep_len: 24.183\n",
      "epoch:  20 \t loss: 24.221 \t return: 27.357 \t ep_len: 27.357\n",
      "epoch:  21 \t loss: 26.827 \t return: 28.749 \t ep_len: 28.749\n",
      "epoch:  22 \t loss: 25.894 \t return: 27.961 \t ep_len: 27.961\n",
      "epoch:  23 \t loss: 25.596 \t return: 28.135 \t ep_len: 28.135\n",
      "epoch:  24 \t loss: 23.072 \t return: 27.059 \t ep_len: 27.059\n",
      "epoch:  25 \t loss: 24.174 \t return: 26.628 \t ep_len: 26.628\n",
      "epoch:  26 \t loss: 25.424 \t return: 26.925 \t ep_len: 26.925\n",
      "epoch:  27 \t loss: 24.714 \t return: 26.204 \t ep_len: 26.204\n",
      "epoch:  28 \t loss: 28.725 \t return: 30.730 \t ep_len: 30.730\n",
      "epoch:  29 \t loss: 25.118 \t return: 27.500 \t ep_len: 27.500\n",
      "epoch:  30 \t loss: 24.608 \t return: 27.070 \t ep_len: 27.070\n",
      "epoch:  31 \t loss: 27.498 \t return: 30.156 \t ep_len: 30.156\n",
      "epoch:  32 \t loss: 25.214 \t return: 28.140 \t ep_len: 28.140\n",
      "epoch:  33 \t loss: 26.843 \t return: 29.273 \t ep_len: 29.273\n",
      "epoch:  34 \t loss: 29.515 \t return: 30.309 \t ep_len: 30.309\n",
      "epoch:  35 \t loss: 29.592 \t return: 33.852 \t ep_len: 33.852\n",
      "epoch:  36 \t loss: 30.120 \t return: 33.296 \t ep_len: 33.296\n",
      "epoch:  37 \t loss: 32.001 \t return: 34.397 \t ep_len: 34.397\n",
      "epoch:  38 \t loss: 31.324 \t return: 34.007 \t ep_len: 34.007\n",
      "epoch:  39 \t loss: 33.535 \t return: 32.922 \t ep_len: 32.922\n",
      "epoch:  40 \t loss: 31.548 \t return: 34.826 \t ep_len: 34.826\n",
      "epoch:  41 \t loss: 34.376 \t return: 37.669 \t ep_len: 37.669\n",
      "epoch:  42 \t loss: 36.695 \t return: 39.732 \t ep_len: 39.732\n",
      "epoch:  43 \t loss: 36.394 \t return: 38.822 \t ep_len: 38.822\n",
      "epoch:  44 \t loss: 33.751 \t return: 38.546 \t ep_len: 38.546\n",
      "epoch:  45 \t loss: 38.604 \t return: 42.761 \t ep_len: 42.761\n",
      "epoch:  46 \t loss: 37.635 \t return: 42.831 \t ep_len: 42.831\n",
      "epoch:  47 \t loss: 37.881 \t return: 41.500 \t ep_len: 41.500\n",
      "epoch:  48 \t loss: 40.349 \t return: 44.805 \t ep_len: 44.805\n",
      "epoch:  49 \t loss: 39.888 \t return: 43.895 \t ep_len: 43.895\n"
     ]
    }
   ],
   "source": [
    "# Create environment for training (no rendering)\n",
    "env = gym.make(\"CartPole-v1\", max_episode_steps=500)\n",
    "env = gym.wrappers.NormalizeObservation(env)\n",
    "env.action_space.seed(seed)\n",
    "\n",
    "print(\"Starting REINFORCE training...\")\n",
    "trained_policy = train(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a76735a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing trained policy and creating GIF...\n",
      "Testing trained policy and creating GIF...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[120]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTesting trained policy and creating GIF...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtest_and_create_gif\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrained_policy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m env.close()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[115]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mtest_and_create_gif\u001b[39m\u001b[34m(policy, gif_filename)\u001b[39m\n\u001b[32m     35\u001b[39m frames.append(Image.fromarray(frame))\n\u001b[32m     37\u001b[39m state_tensor = preprocess_state(state)\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m action_probs = \u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m action = torch.argmax(action_probs, dim=\u001b[32m0\u001b[39m).item()\n\u001b[32m     41\u001b[39m state, reward, done, truncation, _ = test_env.step(action)\n",
      "\u001b[31mTypeError\u001b[39m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "print(\"\\nTesting trained policy and creating GIF...\")\n",
    "test_and_create_gif(trained_policy)\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".deep_rl_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
